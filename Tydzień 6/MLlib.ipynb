{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLlib.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP/FG2HbWThZlmYkIadbRCq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PiotrMaciejKowalski/kurs-analiza-danych-2022/blob/main/Tydzie%C5%84%206/MLlib.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Sparka\n",
        "\n",
        "## Utworzenie środowiska pyspark do obliczeń\n",
        "\n",
        "Tworzymy swoje środowisko z pysparkiem we wenętrzu naszych zasobów chmurowych"
      ],
      "metadata": {
        "id": "WZbf1b9pKx6y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PXLEoH2hKqin"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q ftp.ps.pl/pub/apache/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz"
      ],
      "metadata": {
        "id": "EkQzR_ubK1eC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xf spark-3.1.2-bin-hadoop2.7.tgz"
      ],
      "metadata": {
        "id": "Z87mlTE1K16s"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop2.7\""
      ],
      "metadata": {
        "id": "zoYsMaI4K2OA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark\n",
        "\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "fMQqXLFHLHT7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utworzenie sesji z pyspark\n",
        "\n",
        "\n",
        "Utworzymy testowo sesję aby zobaczyć czy działa. Element ten jest wspólny również gdy systemy sparkowe pracują w sposób ciągły, a nie są tworzone przez naszą sesję."
      ],
      "metadata": {
        "id": "xDc5ZhNOLMWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()"
      ],
      "metadata": {
        "id": "Ve4zuoF4LHc-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Podłączenie Google Drive do sesji colab"
      ],
      "metadata": {
        "id": "wDo7HXvbLPi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eLg-JpwLPt9",
        "outputId": "c6047a2a-e836-4d43-d546-9b131b35281b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pakiet MLlib\n",
        "\n",
        "Choć domyślnym sposobem pracy we współczesnych modelach analizy danych jest budowanie ich natywnie w Pythonie, to czasami nie jest możliwe np. uprodukcyjnienie modelu. Apache Spark jest jednym z narzędzi o największych możliwościach przetwarzania. \n",
        "\n",
        "Spróbujemy jako przykład użyć tu dużo prostszego zbioru jakim jest Iris"
      ],
      "metadata": {
        "id": "jFR21I7FKuRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/iris.data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EEryzNRKuxG",
        "outputId": "f60bd059-3b59-4e17-e11c-d9f4d9d421f3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 21830110_E_Faktura_20211021.pdf\n",
            "'Big Data Roles in Lugano.rtf.gdoc'\n",
            " cdr_d.csv\n",
            "'Colab Notebooks'\n",
            " draw-io\n",
            " flights.csv\n",
            "'Gmail labels.gsheet'\n",
            " iris.data\n",
            " irtOw3miIoGwMAw255_XGvQZm8BS0w0K0yL-Ym_w2tQ.png\n",
            " karta_lokalizacji_pasazera.pdf\n",
            " PART_1650571844111_Resized_20220421_220802.jpeg\n",
            " PART_1650571850277_Resized_20220421_220747.jpeg\n",
            " PART_1650571874607_Resized_20220421_220901.jpeg\n",
            "'PART_1650571907856_Resized_20220421_220930 (1).jpeg'\n",
            " PART_1650571907856_Resized_20220421_220930.jpeg\n",
            " Poland1004_09_13.eu4\n",
            "'Print tickets'\n",
            "'SE calc.ods'\n",
            "'SE calc.ods.gsheet'\n",
            " spark-3.1.1-bin-hadoop2.7\n",
            " spark-warehouse\n",
            " Takeout\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pola_zbiorczo = '''P_LEN,P_WIDTH,S_LEN,S_WIDTH,SPECIES'''\n",
        "pola = pola_zbiorczo.split(',')"
      ],
      "metadata": {
        "id": "12JUJvpEPwjC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StringType, IntegerType, BooleanType, FloatType, TimestampType, DateType, ArrayType, MapType\n",
        "from typing import List, Tuple, Dict, Any\n",
        "map_python_types_2_spark_types = {\n",
        "    str : StringType(),\n",
        "    int : IntegerType(),\n",
        "    bool : BooleanType(),\n",
        "    float: FloatType(),\n",
        "    'timestamp' : TimestampType(),\n",
        "    'date' : DateType(),\n",
        "    List[str] : ArrayType(StringType()),\n",
        "    Tuple[str] : ArrayType(StringType()),\n",
        "    Dict[str, str] : MapType(StringType(), StringType())\n",
        "}\n",
        "\n",
        "column_type_collection = {\n",
        "    float : [ 'P_LEN','P_WIDTH','S_LEN','S_WIDTH' ],\n",
        "    str : [ 'SPECIES' ]\n",
        "}\n",
        "\n",
        "map_column_names_2_types = {}\n",
        "\n",
        "for pole in pola:\n",
        "  for python_type, column_list in column_type_collection.items():\n",
        "    if pole in column_list:\n",
        "      map_column_names_2_types[pole] = map_python_types_2_spark_types[python_type]\n",
        "\n",
        "print(map_column_names_2_types)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMMFVlBqPwp4",
        "outputId": "42eea158-2d96-44cd-c91e-9d96b75578aa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'P_LEN': FloatType, 'P_WIDTH': FloatType, 'S_LEN': FloatType, 'S_WIDTH': FloatType, 'SPECIES': StringType}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "schemat = StructType()\n",
        "for pole, typ in map_column_names_2_types.items():\n",
        "    schemat = schemat.add(pole, typ, True)"
      ],
      "metadata": {
        "id": "BIV7GOz1QIGW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris = spark.read.format('csv').option(\"header\", False).schema(schemat).load('/content/drive/MyDrive/iris.data')\n",
        "iris.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQXR1flBPwvk",
        "outputId": "32912238-0ac8-4716-9dee-dcd846245aaa"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------+-----+-------+-----------+\n",
            "|P_LEN|P_WIDTH|S_LEN|S_WIDTH|    SPECIES|\n",
            "+-----+-------+-----+-------+-----------+\n",
            "|  5.1|    3.5|  1.4|    0.2|Iris-setosa|\n",
            "|  4.9|    3.0|  1.4|    0.2|Iris-setosa|\n",
            "|  4.7|    3.2|  1.3|    0.2|Iris-setosa|\n",
            "|  4.6|    3.1|  1.5|    0.2|Iris-setosa|\n",
            "|  5.0|    3.6|  1.4|    0.2|Iris-setosa|\n",
            "+-----+-------+-----+-------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iris.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdIRm4YxMLka",
        "outputId": "40fd178e-f093-4710-978c-7977764d306c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- P_LEN: float (nullable = true)\n",
            " |-- P_WIDTH: float (nullable = true)\n",
            " |-- S_LEN: float (nullable = true)\n",
            " |-- S_WIDTH: float (nullable = true)\n",
            " |-- SPECIES: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing \n",
        "\n",
        "Dla pełnego wykorzystania zbioru IRIS użyjemy transformacji liczbowej na kolumnie SPECIES aby uzyskać dostęp do wszystkich modeli analitycznych"
      ],
      "metadata": {
        "id": "c9r6QI44SR9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "types = iris.select('SPECIES').distinct().toPandas().values.tolist()\n",
        "types"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YC16qsLqSRRO",
        "outputId": "1079f4d4-c4f7-468b-8558-02610d4208c9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Iris-virginica'], ['Iris-setosa'], ['Iris-versicolor']]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "map_type = [{ typ[0]: i for i, typ in zip(range(3),types) }]\n",
        "convert_species = spark.createDataFrame(data=map_type, schema = ['type', 'species_code'])\n",
        "convert_species.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qAiwoKdQEXy",
        "outputId": "5f197e39-db0a-4721-cee4-4a3681e6d5a9"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------------+--------------+\n",
            "|type|species_code|Iris-virginica|\n",
            "+----+------------+--------------+\n",
            "|   1|           2|             0|\n",
            "+----+------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataDictionary = [\n",
        "        ('James',{'hair':'black','eye':'brown'}),\n",
        "        ('Michael',{'hair':'brown','eye':None}),\n",
        "        ('Robert',{'hair':'red','eye':'black'}),\n",
        "        ('Washington',{'hair':'red','eye':'grey'}),\n",
        "        ('Jefferson',{'hair':'red','eye':''})\n",
        "        ]\n",
        "df = spark.createDataFrame(data=dataDictionary, schema = [\"name\",\"properties\"])\n",
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJqVo5TDTd61",
        "outputId": "65ec1a89-b02c-443f-ad87-b952c6f86b50"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- properties: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            "\n",
            "+----------+-----------------------------+\n",
            "|name      |properties                   |\n",
            "+----------+-----------------------------+\n",
            "|James     |{eye -> brown, hair -> black}|\n",
            "|Michael   |{eye -> null, hair -> brown} |\n",
            "|Robert    |{eye -> black, hair -> red}  |\n",
            "|Washington|{eye -> grey, hair -> red}   |\n",
            "|Jefferson |{eye -> , hair -> red}       |\n",
            "+----------+-----------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "q5VTI3-3VrpG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}